% Emacs, this is -*-latex-*-

\title{Quantization}

\author{Vicente Gonz√°lez Ruiz}

\maketitle

\section{\href{https://en.wikipedia.org/wiki/Quantization\_(signal\_processing)}{Basics}}
\begin{itemize}
\tightlist

\item
  A quantizer discretizes the amplitude of a
  \href{https://en.wikipedia.org/wiki/Pulse-amplitude_modulation}{PAM
    signal} \(s(nT_s)\) (where $s$ is an analog signal,
  $n\in{\mathbb{Z}}$ and $T_s$ is sampling period), producing an
  analog
  \href{https://en.wikipedia.org/wiki/Pulse-code_modulation}{PCM
    signal} $s[n]$. Therefore, quantization maps the 
  values of samples into a discrete set of values
  \cite{vetterli1995wavelets}.

\item
  The quantization process can be modeled as
  \begin{equation}
    s[n] = s(nT_s) + e(nT_s),
  \end{equation}
  being \(e(nT_s)\) the quantization error (also called, quantization
  noise). Therefore, quantization produces a loss of information
  because we cannot recover $s(nT_s)$ from $s[n]$.

\item
  Quantizers are defined from their set of \(d_i; i\in {\mathbb{Z}}\)
  (decision levels) and \(r_i; i\in {\mathbb{Z}}\) (representation
  levels). $\{r_i\}$ must be finite.

\item Another design parameter of quantizers is the definition of the
  decision boundaries $d_{\text min}$ and $d_{\text max}$, ($d_{\text min}<d_{\text max}$) that define the low and high
  overload regions as
  \begin{equation}
    s[n] = \{\begin{array}{ll}
    r_{\text min} & \text{if $s(nT_s)<d_{\text min}$} \\
    r_{\text max} & \text{if $s(nT_s)>d_{\text max}$} \\
    s(nT_s)+e(nT_s) & \text{otherwise}.
    \end{array}
  \end{equation}

\end{itemize}

\section{Scalar quantization}
\begin{itemize}
\item Lets denote $x=\{s_(nT_s)\}$ as the inputs of the quantizer and
  $y=\{s[n]\}$ as the outputs.

\item In a scalar quantizer, each input sample $x_i$ is individually
  quantized as $y_i$, and the input range is divided into intervals
  \begin{equation}
    I_i=(d_{i-1}, d_i].
  \end{equation}
  
\item The performance of the quantizer is measured as the distance
  between the input $x$ and the output $y$
  \begin{equation*}
    d(x,y) = |x-y|^2.
  \end{equation*}
  Thus, the MSE is
  \begin{equation}
    \text{MSE} = E(|x-y|^2)=\sum_i\int_{x_{i-1}}^{x_i} (x-y_i)^2f_X(x)dx,
  \end{equation}
  where $d_X(x)$ is the probability density function (PDF) of $x$ and
  $E(\cdot)$ the expectation.

\item The quantization error
  \begin{equation}
    e=y-x
  \end{equation}
  is modeled as a noise process
  which: (1) is uncorrelated to the input, (2) is white, and (3)
  follows a uniform distribution. This is a fair approximation only if
  $\Delta<<\sigma_x$~\cite{vetterli1995wavelets}.

\end{itemize}

\section{Uniform (lineal) scalar quantization}
\begin{itemize}
\item
  In a linear
  quantizer, the quantization step \(\Delta\) satisfies that
  \begin{equation}
    \Delta=d_{i+1}-d_i=r_{i+1}-r_i, \forall i.
  \end{equation}
  Notice that, for a given dynamic range of \(s\), \(Q\) is inversely
  proportional to \(\Delta\), and viceversa.

\item
  In uniform quantizers, \(\Delta\) does not depends on the PAM sample
  values. Notice that in this quantizer,
  $e(nTs)_{\text{max}}=\frac{\Delta}{2}$.  This is a
  $q=\lceil\log_2(5)\rceil=3$-bits quantizer $(Q=8)$.

\item Besides $\Delta$, another design parameter of quantizers is the
  definition of the boundaries $a$ and $b$, ($a<b$) that define the
  low ($a$) and high ($b$) overload regions. % Ojo, medio duplicado
  
\item
  Uniform quantizers are used in most A/D (analogic/digital)
  converters, were it is expected the generation of uniformely
  distributed sequences of samples.

\item
  For the simple, the input intervals are of the form $(i-1/2,i+1/2]$
    and $r_i=i$.

\item Under the premise that $e$ is uniform, and considering that
  $y_i=(x_{i-1}+x_i)/2$ (something quite reasonable when $x$ can be
  considered also uniform, the average quantization error is $Z/4$
  ($Z/2$ is the meximum and $0$ is the minimum), and for this
  particular case
  \begin{equation}
    \text{MSE} = \frac{1}{\Delta}\int_{-\Delta/2}^{\Delta/2}e^2de=\frac{\Delta^2}{12}.
  \end{equation}

\item Considering that $\Delta=\frac{b-a}{Q}$, $\Delta$ decreases as
  $\frac{1}{Q}$.

\item El error de cuantificaci\'on promedio en un cuantificador lineal
  depende del paso de cuantificaci\'on y es igual a $Z/4$ ($Z/2$ es el
  m\'aximo y $0$ el m\'inimo). Por tanto,
  \begin{equation*}
    \overline{\text{SNR}}=\frac{\displaystyle\sum_{n=1}^N
    s(nT_s)^2}{\displaystyle\sum_{n=1}^N (Z/4)^2} =
    \frac{\displaystyle\sum_{n=1}^N
      s(nT_s)^2}{N\displaystyle\frac{Z^2}{16}}.
  \end{equation*}

  \item Como se puede apreciar de esta expresi\'on, la relaci\'on
  se\~nal/ruido promedio es directamente proporcional a la inversa del
  paso de cuantificaci\'on elevado a cuadrado, es decir,
\begin{equation*}
  \overline{\text{SNR}}\sim \frac{1}{Z^2}.
\end{equation*}
\item Si usamos PCM (Puse Code Modulation)\footnote{Sistemas de
  modulaci\'on digital que estudiaremos en posteriormente.} para
  representar las muestras cuantificadas $s[n]$, se cumple
\begin{equation*}
  Z\sim\frac{1}{2^b},
\end{equation*}
donde $b$ es el n\'umero de bits utilizados para representar los niveles
de representaci\'on. Es decir, a mayor n\'umero de bits, menor paso de
cuantificaci\'on.
\item Por tanto,
  \begin{equation*}
    \overline{\text{SNR}}\sim (2^b)^2 = 2^{2b}.
  \end{equation*}
\item Finalmente, ``tomando Decibelios'',
   \begin{equation*}
    \overline{\text{SNR}_{\text{dB}}}\sim 10\log_{10} 2^{2b} = 20b\log_{10} 2
    \approx 6b~\text{dB}.
  \end{equation*}

\item Este resultado es interesante porque indica que, en un sistema
  PCM, con cada bit de precisi\'on mejoramos la calidad de la se\~nal
  digitalizada en un factor de $6$ dB. Si consideramos que para
  alcanzar una calidad HiFi (High Fidelity) en audio debemos conseguir
  $96$ dB de relaci\'on se\~nal/ruido, deberemos usar
  \begin{displaymath}
    \frac{96}{6} = 16~\text{bits/muestra},
  \end{displaymath}
  valor que coincide con la resoluci\'on usada en los CDs de audio!
  
\end{itemize}

\subsection{Using codewords (encoding)}
\begin{itemize}
  \tightlist
  
\item
  Depending on the number of \(Q\) different possible values (or
  \emph{bins}) for \(s[\cdot]\), we speak of a
  \(q=\lceil\log_2(Q)\rceil\)-bits quantizer (this means that the output
  of the quantizer are \(q\) bits for each sample, or that we have
  \(2^q\) representation levels).

\item
  When we quantize digital signals, these are sequences of digital
  samples represented by symbols of a given alphabet, typically, a
  subset of \({\mathbb{Z}}\) or \({\mathbb{N}}\). Therefore, both the
  input and the output of the quantizer are indexes, not real values
  of a sampled signal. The set $\{r_i\}$ is called the codebook and
  the $r_i$ the codewords.

\item If $R$ is the number of bits of the quantizer, $\Delta$
  decreases as $\frac{1}{2^R}$, and
  \begin{equation}
    \text{MSE} = \frac{(b-a)^2}{12Q^2} = \sigma^22^{-2R}=C2^{-2R}.
  \end{equation}
  since $\sigma^2=\frac{(b-a)^2}{12}$ for uniform input PDF. 
%\item
%  Therefore, in this context, \(\Delta\) represents the length of the
%  intervals of indexes what will be ignored.
\end{itemize}

\subsection{Example}
Quantize
\href{https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg}{Jfk\_berlin\_address\_high.ogg}
using \(\Delta=2\). Compute the variance of both audio sequences.

\subsection*{Example (uniform quantization)}
\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/uniform_quantization.ipynb}{IPython notebook}

\section{Non-uniform quantization}
\begin{itemize}
  \tightlist
\item
  In order to minimize the maximun, average or the total quantization
  error, \(\Delta\) can be adapted to the characteristics of \(s\).
\end{itemize}

\subsection{Companded quantization}
\begin{itemize}
\item
  Non-uniform quantizer.
\item
  \href{https://en.wikipedia.org/wiki/Companding}{Companding}:
  COMpressing + exPANDING. The original signal is mapped through a
  compressor, quantized using an uniform quantized, and re-mapped using
  the corresponding expander. The result is a logarithmic quantization.
\item
  \href{https://en.wikipedia.org/wiki/\%CE\%9C-law_algorithm}{\(\mu\)-law}
  example:
\end{itemize}

\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/companded_quantization.ipynb}{Ipython notebook}

\svg{uniform}{600}
\svg{ulaw-compressor}{600}
\svg{ulaw-expander}{600}
\svg{companded}{600}

\section{PDF-optimized quantization}
\begin{itemize}
\item
  Non-uniform quantizer.
\item
  if we known the probability distribution of the samples, we can select
  a small \(\Delta\) for the most probable samples and viceversa.
\end{itemize}

\svg{cuantif_max-lloyd}{600}

\section{Adaptive quantization}
\begin{itemize}
\item
  Useful when the characteristics of \(s\) (the variance, for example)
  vary over time.
\item
  Typically, the quantizer varies \(\Delta\) depending on such
  characteristics.
\end{itemize}

\section{Forward adaptive quantization}
\begin{itemize}
\item
  Used for determining a suitable \(\Delta\) for blocks of samples.
\item ~
  \hypertarget{encoder}{%
  \subsubsection{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While samples in \(s\):

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read into \(b\) the next \(B\) samples of \(s\).
    \item
      Determine \(\Delta\), minimizing the quantization error, and
      output \(\Delta\) (or the data necessary for its determination).
    \item
      Quantize \(b\) and output it.
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While data in input:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read \(\Delta\) (or the data necessary for determining it, and in
      this case, use the same algorithm that the used by the encoder).
    \item
      ``Dequantize'' \(b\) and output it (note that the dequantization
      is only a way of calling the process of reverting the original
      range of the quantized signal).
    \end{enumerate}
  \end{enumerate}
\item
  The selection of \(B\) is a trade-off between the increase in side
  information needed by small block sizes and the loss of fidelity due
  to large block sizes.
\item
  Forward adaptive quantization generates a
  \(B\text{-samples}\times f_s\) delay (buffering), where \(f_s\) is the
  sampling rate of \(s\).
\end{itemize}

\section{Backward adaptive quantization}
\begin{itemize}
\item
  Only the previously quantized samples are available to use in adapting
  the quantizer.
\item
  Idea: If happens that \(\Delta\) is smaller than it should be, the
  input will fall in the outer levels of the quantizer a high number of
  times. On the other hand, if \(\Delta\) is larger than it should be,
  the samples will fall in the inner levels a high number of times.
\item ~
  \hypertarget{encoder}{%
  \subsubsection{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(s\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Quantize the next sample.
    \item
      Observe the output and refine \(\Delta\).
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(\hat{s}\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      ``Dequantize'' the next sample.
    \item
      Step 2.B of the encoder.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

\section{The Jayant quantizer~\cite{jayant1974digital}}
\begin{itemize}
\item
  Adaptive quantization with a one word memory (\(\Delta_{(t-1)}\)).
\item
  A Jayant quantider defines the Step 2.B. as: Define a multiplier
  \(M_l\) for each quantization level \(l\), where for the inner levels
  \(M_l<1\) and for the outer levels \(M_l>1\), and compute:

  \[
    \Delta^{[n]} = \Delta^{[n-1]}{M_l}^{[n-1]},
  \]

  where \(\Delta^{[n-1]}\) was the previous quantization step and
  \({M_l}^{[n-1]}\) the level multiplier for the \(n-1\)-th (previous)
  sample. Thus, if the previous (\(n-1\)) quantization used a
  \(\Delta^{[n-1]}\) too small (using outer quantization levels) then
  \(\Delta^{[n]}\) will be larger and viceversa.
\item
  Depending on the multipliers \(M\), the quantizer will converge or
  oscillate. In the first case, the quantizer will be good for small
  variations of \(s\) but bad when a fast adaption to large changes in
  \(s\) is required. In the second one, the quantizer will adapt quickly
  to fast variations of \(s\) but will oscillate when \(s\) changles
  slowly.
\item
  Most Jayant quantizers clip the computation of \(\Delta\) to avoid
  generating a zero output quantizer in those contexts where \(s\) is
  zero or very close to zero, and to improve the adaptation to smaller
  samples after a sequence of bigger ones (avoiding to grow without
  limit):

  \[
  \begin{array}{ll}
    \text{if}~\Delta^{[n]}<\Delta_{\text{min}}~\text{then}~\Delta^{[n]} = \Delta_{\text{min}},\\
    \text{if}~\Delta^{[n]}>\Delta_{\text{max}}~\text{then}~\Delta^{[n]} = \Delta_{\text{max}}.
  \end{array}
  \]
\end{itemize}

\section{Adapting with a scale factor}
\begin{itemize}
\item
  A Jayant quantized adapts the quantization step to the dynamic range
  of the signa using a set of multipiers. A similar effect can be
  provided by dividing the input signal by a scale factor defined
  iteratively as:

  \begin{equation}
    \alpha^{[n]} = \alpha^{[n-1]}M_l^{[n-1]}.
  \end{equation}
\end{itemize}

\subsection{Example}
Quantize
\href{https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg}{Jfk\_berlin\_address\_high.ogg}
using \(4\)-bits backward adaptive Jayant quantizer. Reproduce the
quantized sequence and provide a subjective comparison with the original
sequence.

\section{Vector quantization}
\begin{itemize}
\item Samples are quantized in groups (vectors).
\end{itemize}
  

\bibliography{quantization,DWT}
